{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBdvRSszZQM2NAuqne7Rfy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4819890f81c4f4290abd97d7a1a054b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36beda90a5f14c4588f17d68fc48215c",
              "IPY_MODEL_608d9f01ceca4d2a88e237593b8f6ebf",
              "IPY_MODEL_8d06a2acb0214e589fba41b986eebe07"
            ],
            "layout": "IPY_MODEL_a2402db6214f443ab3dec44222abf3ba"
          }
        },
        "36beda90a5f14c4588f17d68fc48215c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb74cceb53e489eb6959f18fa3bb57f",
            "placeholder": "​",
            "style": "IPY_MODEL_dcb7e83d40f44c378e8b6483b6fd1261",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "608d9f01ceca4d2a88e237593b8f6ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c1a9ad4941496b85234469a7526a5f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_049c46c59bbd456e87fe6ad81c529acd",
            "value": 3
          }
        },
        "8d06a2acb0214e589fba41b986eebe07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122e1abb88124360b6e13e5dd8a436ae",
            "placeholder": "​",
            "style": "IPY_MODEL_fb2929d804c04a41831262e6dc082e8b",
            "value": " 3/3 [00:57&lt;00:00, 16.29s/it]"
          }
        },
        "a2402db6214f443ab3dec44222abf3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb74cceb53e489eb6959f18fa3bb57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcb7e83d40f44c378e8b6483b6fd1261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07c1a9ad4941496b85234469a7526a5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049c46c59bbd456e87fe6ad81c529acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "122e1abb88124360b6e13e5dd8a436ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2929d804c04a41831262e6dc082e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NN198/LLM-Magic/blob/main/Generating_Detecting_Texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpdclx6uwiH_",
        "outputId": "36784db5-f7cb-4393-83b2-3e2500515bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePvWe1C-F16V"
      },
      "outputs": [],
      "source": [
        "#Importing used packages and libraries\n",
        "from transformers import AutoConfig, AutoTokenizer,AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import openai #to use OpenAI api\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import auc,roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another series of work attempted to build connections\n",
        "# between ICL and gradient descent. Taking\n",
        "# linear regression as a starting point, Akyürek\n",
        "# et al. (2022) found that Transformer-based incontext\n",
        "# learners can implement standard finetuning\n",
        "# algorithms implicitly, and von Oswald et al.\n",
        "# (2022) showed that linear attention-only Transformers\n",
        "# with hand-constructed parameters and models\n",
        "# learned by gradient descent are highly related."
      ],
      "metadata": {
        "id": "hWQbCXH5HY1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Instatiating the model for detection purposes"
      ],
      "metadata": {
        "id": "v7adjut6QlP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vicuna-7B Model detector\n",
        "*has been adversarially trained and known to produce best detection between human and AI text*"
      ],
      "metadata": {
        "id": "lLzi62YmSqFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming detector is your model and tokenizer is already defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cuda:0\"\n",
        "detector_modelcard = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
        "#models - [Writer/camel-5b-hf,databricks/dolly-v2-3b]\n",
        "detector = AutoModelForSequenceClassification.from_pretrained(detector_modelcard)"
      ],
      "metadata": {
        "id": "ffsHzTLUL_Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pretrained model classes and configuration*"
      ],
      "metadata": {
        "id": "eCMkt41KP2kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(detector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06O28GXfOlcg",
        "outputId": "db59228f-1843-4a6c-fe47-ee0899016dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaForSequenceClassification(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-23): 24 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHead(\n",
            "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Configuration**"
      ],
      "metadata": {
        "id": "T4fyCRVTQcPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detector_config = AutoConfig.from_pretrained(detector_modelcard)\n",
        "print(detector_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJfP_x9fO1Lt",
        "outputId": "4d9e018c-744a-490f-e788-7d8e8bd79e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"TrustSafeAI/RADAR-Vicuna-7B\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tokenizing the model weights for performing paraphrasing and processing the ai texts before calculating the predictions*"
      ],
      "metadata": {
        "id": "6W8XburxTLZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(detector_modelcard)\n",
        "detector.eval()\n",
        "# detector.to(device)"
      ],
      "metadata": {
        "id": "mOqK-8YXPeSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bc7999-05ef-46c9-e791-14c8ef9e85fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Completing the given prompts using different pretrained gpt models to test the robustness of the detector"
      ],
      "metadata": {
        "id": "6uiwZcm16Cfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction and text inputs (Paper Reference -[Learning toWatermark LLM-generated Text via Reinforcement Learning] \"Examples of C4 on Llama2-7b dataset\", Examples of C4 on OPT-1.3b dataset)\n",
        "instruction = \"You will assist in completing the given text examples:\"\n",
        "raw_texts = [\"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald...\",\n",
        "             \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications...\",\n",
        "             \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take...\",\n",
        "             \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent...\"]\n",
        "human_texts = [\"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald Trump and North Korean leader Kim Jong Un will meet within weeks, White House spokesman Raj Shah said on Monday, as the administration seeks a deal with Pyongyang to rid North Korea of its nuclear weapons programme. The meeting would be the first of its kind between leaders of the countries, and potentially set in motion a long-awaited ”peace dialogue” that could lead to the first meeting of Moon Jae-in, the newly elected leader of South Korea, with the reclusive North. Advertisement For the latest headlines, follow our Google News channel online or via the app. The visit of Kim to South Korea for a summit on April\",\n",
        "               \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "               \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take a bite out of the cost of living in the South Bay. The bill, AB 191, would allow cities to create a Community Facilities District, which would allow them to issue bonds to pay for infrastructure improvements. The bill would also allow cities to create a Community Facilities District to pay for infrastructure improvements. “The South Bay is a great place to live, work and raise a family, but the cost of living is too high,” Muratsuchi said in a statement. “AB 191 will help cities in the South Bay and across the state\",\n",
        "               \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"]"
      ],
      "metadata": {
        "id": "SEbZnVMCvPwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLGClass:\n",
        "  def __init__(self, api_key, raw_texts, instruction, model):\n",
        "    openai.api_key = api_key\n",
        "    self.human_texts = raw_texts\n",
        "    self.instruction = instruction\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def generate_text_completion(self, max_tokens, seed, temperature,top_p, n, frequency_penalty):\n",
        "    ai_texts = []\n",
        "    for text in self.human_texts:\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model=self.model,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": self.instruction},\n",
        "              {\"role\": \"user\", \"content\": text}\n",
        "          ],\n",
        "          max_tokens=max_tokens,\n",
        "          seed=seed,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          n=n, #iterations\n",
        "          stop=None,\n",
        "          frequency_penalty=frequency_penalty\n",
        "      )\n",
        "      ai_texts.append(response.choices[0].message['content'].strip())\n",
        "    return ai_texts"
      ],
      "metadata": {
        "id": "zpv5r_db9oXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating completed texts from GPT-3.5 turbo"
      ],
      "metadata": {
        "id": "knlfITKBWgb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"your-openai-key\"\n",
        "model = \"gpt-3.5-turbo\"\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=0, temperature=0.5,top_p=0.1, n=1, frequency_penalty=0)\n",
        "print(AI_texts)"
      ],
      "metadata": {
        "id": "OENehoUmOTD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paraphrasing human responses"
      ],
      "metadata": {
        "id": "IYzAnnrf6Prm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParaphraseNLG:\n",
        "  @staticmethod\n",
        "  def openai_response(text, openai_model):\n",
        "    instruction = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    prompt={\"role\": \"user\", \"content\": text}\n",
        "    messages = [instruction, prompt]\n",
        "    k_wargs = { \"messages\": messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r"
      ],
      "metadata": {
        "id": "4USrWrl1PZ1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Paraphrasing human text to worsen the quality of the text that can be compared with the AI generated texts\n",
        "class ParaphraseNLGHumanText:\n",
        "  @staticmethod\n",
        "  def openai_response(text, openai_model):\n",
        "    instruction = {\"role\": \"system\", \"content\": \"Worsen the word choices in the sentence to sound less like that of a human\"}\n",
        "    prompt={\"role\": \"user\", \"content\": text}\n",
        "    messages = [instruction, prompt]\n",
        "    k_wargs = { \"messages\": messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r"
      ],
      "metadata": {
        "id": "xVmIa7ZJqKeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the paraphrased responses generated from GPT 3.5\n",
        "#Using gpt-4o to check the semantical differences from human text\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "Paraphrased_responses_ht = [ParaphraseNLGHumanText.openai_response(i, \"gpt-4o\") for i in human_texts]"
      ],
      "metadata": {
        "id": "zRQoEL_aTTrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the log-probabilities for each instance in the batch"
      ],
      "metadata": {
        "id": "ZcsfZ4I9lzF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#without gradient\n",
        "# input will contain the raw output scores(logits in this case) of the probabilities generated by the detector\n",
        "class textPrediction:\n",
        "  @staticmethod\n",
        "  def textInputFun(examples):\n",
        "    with torch.no_grad():\n",
        "      inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "      input = {k: v.to(device) for k, v in inputs.items()}\n",
        "      output_prob = F.log_softmax(detector(**inputs).logits, -1)[:, 0].exp().tolist()\n",
        "    print(\"No of input instances fed to the detector\", len(examples))\n",
        "    print(f\"Probability of text examples: {output_prob}\")\n",
        "    return output_prob"
      ],
      "metadata": {
        "id": "77hf-_HLbtVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using gradients\n",
        "class textPredictionwGrad:\n",
        "    @staticmethod\n",
        "    def textInputFun(examples):\n",
        "        inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        # Ensure float tensors have requires_grad set to True\n",
        "        for key, tensor in inputs.items():\n",
        "            if tensor.dtype in [torch.float, torch.float32, torch.float64]:\n",
        "                tensor.requires_grad_(True)\n",
        "\n",
        "        # Ensure all inputs are on the correct device\n",
        "        for key, tensor in inputs.items():\n",
        "            inputs[key] = tensor.to(device)\n",
        "\n",
        "        outputs = detector(**inputs)\n",
        "        logits = outputs.logits\n",
        "        output_prob = F.log_softmax(logits, -1)[:, 0].exp().tolist()\n",
        "        print(\"No of input instances fed to the detector\", len(examples))\n",
        "        print(f\"Probability of text examples: {output_prob}\")\n",
        "        return output_prob"
      ],
      "metadata": {
        "id": "zU4NUZJyzBlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Without gradients\n",
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(human_texts)\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)\n",
        "print(\"Paraphrased human text predictions\")\n",
        "paraphrased_preds_ht = textPrediction.textInputFun(Paraphrased_responses_ht)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfi4x-ycqHFt",
        "outputId": "b87a1c6d-90bd-4906-f2e2-fd3f453a4e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.22256025671958923, 0.1006336361169815, 0.9964590668678284, 0.0139209795743227]\n",
            "AI text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9994514584541321, 0.9986514449119568, 0.9993454813957214, 0.9992238283157349]\n",
            "Paraphrased text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9993172883987427, 0.997180700302124, 0.9993402361869812, 0.9991400241851807]\n",
            "Paraphrased human text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.3398597240447998, 0.034741152077913284, 0.9979898929595947, 0.012674978002905846]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using gradients to determine the log probabilities"
      ],
      "metadata": {
        "id": "3ygOvL1bLMfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using gradients\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "detector_modelcard = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
        "detector = AutoModelForSequenceClassification.from_pretrained(detector_modelcard)\n",
        "tokenizer = AutoTokenizer.from_pretrained(detector_modelcard)\n",
        "detector.eval().to(device)\n",
        "\n",
        "# Instruction and text inputs\n",
        "instruction = \"You will assist in completing the given text examples:\"\n",
        "raw_texts = [\n",
        "    \"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald...\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications...\",\n",
        "    \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take...\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent...\"\n",
        "]\n",
        "human_texts = [\n",
        "    \"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald Trump and North Korean leader Kim Jong Un will meet within weeks, White House spokesman Raj Shah said on Monday, as the administration seeks a deal with Pyongyang to rid North Korea of its nuclear weapons programme. The meeting would be the first of its kind between leaders of the countries, and potentially set in motion a long-awaited ”peace dialogue” that could lead to the first meeting of Moon Jae-in, the newly elected leader of South Korea, with the reclusive North. Advertisement For the latest headlines, follow our Google News channel online or via the app. The visit of Kim to South Korea for a summit on April\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "    \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take a bite out of the cost of living in the South Bay. The bill, AB 191, would allow cities to create a Community Facilities District, which would allow them to issue bonds to pay for infrastructure improvements. The bill would also allow cities to create a Community Facilities District to pay for infrastructure improvements. “The South Bay is a great place to live, work and raise a family, but the cost of living is too high,” Muratsuchi said in a statement. “AB 191 will help cities in the South Bay and across the state\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"\n",
        "]\n",
        "\n",
        "class NLGClass:\n",
        "    def __init__(self, api_key, raw_texts, instruction, model):\n",
        "        openai.api_key = api_key\n",
        "        self.human_texts = raw_texts\n",
        "        self.instruction = instruction\n",
        "        self.model = model\n",
        "\n",
        "    def generate_text_completion(self, max_tokens, seed, temperature, top_p, n, frequency_penalty):\n",
        "        ai_texts = []\n",
        "        for text in self.human_texts:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.instruction},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "                max_tokens=max_tokens,\n",
        "                seed=seed,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                n=n,  # iterations\n",
        "                stop=None,\n",
        "                frequency_penalty=frequency_penalty\n",
        "            )\n",
        "            ai_texts.append(response.choices[0].message['content'].strip())\n",
        "        return ai_texts\n",
        "\n",
        "api_key = \"your-openai-key\"\n",
        "model = \"gpt-3.5-turbo\"\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=0, temperature=0.5, top_p=0.1, n=1, frequency_penalty=0)\n",
        "print(AI_texts)\n",
        "\n",
        "class ParaphraseNLG:\n",
        "  @staticmethod\n",
        "  def openai_response(text, openai_model):\n",
        "    instruction = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    prompt={\"role\": \"user\", \"content\": text}\n",
        "    messages = [instruction, prompt]\n",
        "    k_wargs = { \"messages\": messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r\n",
        "\n",
        "# Paraphrasing human text to worsen the quality of the text that can be compared with the AI generated texts\n",
        "class ParaphraseNLGHumanText:\n",
        "    @staticmethod\n",
        "    def openai_response(text, openai_model):\n",
        "        instruction = {\"role\": \"system\", \"content\": \"Worsen the word choices in the sentence to sound less like that of a human\"}\n",
        "        prompt = {\"role\": \"user\", \"content\": text}\n",
        "        messages = [instruction, prompt]\n",
        "        k_wargs = {\"messages\": messages, \"model\": openai_model}\n",
        "        r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "        return r\n",
        "\n",
        "# Using gradients\n",
        "class textPredictionwGrad:\n",
        "    @staticmethod\n",
        "    def textInputFun(examples):\n",
        "        inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Ensure float tensors have requires_grad set to True\n",
        "        for key, tensor in inputs.items():\n",
        "            if tensor.dtype in [torch.float, torch.float32, torch.float64]:\n",
        "                tensor.requires_grad_(True)\n",
        "\n",
        "        # Ensure all inputs are on the correct device\n",
        "        for key, tensor in inputs.items():\n",
        "            inputs[key] = tensor.to(device)\n",
        "\n",
        "        outputs = detector(**inputs)\n",
        "        logits = outputs.logits\n",
        "        output_prob = F.log_softmax(logits, -1)[:, 0].exp().tolist()\n",
        "        print(\"No of input instances fed to the detector\", len(examples))\n",
        "        print(f\"Probability of text examples: {output_prob}\")\n",
        "        return output_prob\n",
        "\n",
        "# Using gradients\n",
        "print(\"Human text predictions from the detector using gradients\")\n",
        "human_preds_wg = textPredictionwGrad.textInputFun(human_texts)\n",
        "\n",
        "print(\"AI text predictions from the detector using gradients\")\n",
        "ai_preds_wg = textPredictionwGrad.textInputFun(AI_texts)\n",
        "\n",
        "# Assuming Paraphrased_responses and Paraphrased_responses_ht are defined somewhere in your code\n",
        "# Example placeholders\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "Paraphrased_responses_ht = [ParaphraseNLGHumanText.openai_response(i, \"gpt-4o\") for i in human_texts]\n",
        "\n",
        "print(\"Paraphrased text predictions using gradients\")\n",
        "paraphrased_preds_wg = textPredictionwGrad.textInputFun(Paraphrased_responses)\n",
        "\n",
        "print(\"Paraphrased human text predictions using gradients\")\n",
        "paraphrased_preds_ht_wg = textPredictionwGrad.textInputFun(Paraphrased_responses_ht)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCnp8aVc8vkT",
        "outputId": "41e3ffd9-6ce2-40ff-f1b7-db321e25e621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Trump has expressed his willingness to meet with North Korean leader Kim Jong Un in the near future.', 'such as steam generation, water heating, and thermal energy storage. These technologies use mirrors or lenses to concentrate sunlight onto a receiver, where the energy is converted into heat. This heat can then be used in various industrial processes such as food processing, chemical production, and desalination. By harnessing the power of the sun, these solar concentrating technologies offer a sustainable and environmentally friendly alternative to traditional fossil fuel-based heating systems.', '...action against offshore oil drilling in California. The bill aims to protect the coastal communities and marine environment from the potential risks associated with oil drilling activities.', 'The Bush administration then turned its attention to Iraq, arguing that the need to remove Saddam Hussein from power had become urgent due to concerns about weapons of mass destruction and links to terrorism.']\n",
            "Human text predictions from the detector using gradients\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.22256092727184296, 0.10063392668962479, 0.9964591264724731, 0.013920946978032589]\n",
            "AI text predictions from the detector using gradients\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9994514584541321, 0.9984751343727112, 0.9994996786117554, 0.9985182881355286]\n",
            "Paraphrased text predictions using gradients\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9992637634277344, 0.995789110660553, 0.9994179010391235, 0.9954054355621338]\n",
            "Paraphrased human text predictions using gradients\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.22057388722896576, 0.027686314657330513, 0.9983727335929871, 0.00977335311472416]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roc_metrics(human_preds, ai_pred):\n",
        "      # human_preds is the ai-generated probabilities of human-text\n",
        "      # ai_preds is the ai-generated probabiities of ai-text\n",
        "      fpr, tpr, _ = roc_curve([0] * len(human_preds_wg) + [1] * len(ai_preds_wg), human_preds_wg + ai_preds_wg,pos_label=1)\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      return fpr.tolist(), tpr.tolist(), float(roc_auc)"
      ],
      "metadata": {
        "id": "cBfVTRJOqFWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W/O Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,ai_preds))\n",
        "print(\"W/ Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,paraphrased_preds))\n",
        "print(\"W/ Paraphrase Detection AUROC: \", get_roc_metrics(human_preds,paraphrased_preds_ht))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGyFU_EZnwc8",
        "outputId": "061fef4a-b966-42be-c485-4fd7f52c62b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W/O Paraphrase Detection AUROC:  ([0.0, 0.0, 0.0, 1.0], [0.0, 0.25, 1.0, 1.0], 1.0)\n",
            "W/ Paraphrase Detection AUROC:  ([0.0, 0.0, 0.0, 0.25, 0.25, 1.0], [0.0, 0.25, 0.75, 0.75, 1.0, 1.0], 0.9375)\n",
            "W/ Paraphrase Detection AUROC:  ([0.0, 0.0, 0.25, 0.25, 1.0, 1.0], [0.0, 0.25, 0.25, 0.5, 0.5, 1.0], 0.4375)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"W/O Paraphrase Detection AUROC with gradients: \", get_roc_metrics(human_preds_wg,ai_preds_wg))\n",
        "print(\"W/ Paraphrase Detection AUROC with gradients: \", get_roc_metrics(human_preds_wg,paraphrased_preds_wg))\n",
        "print(\"W/ Paraphrase Detection AUROC with gradients: \", get_roc_metrics(human_preds_wg,paraphrased_preds_ht))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpFdJTWt-EVJ",
        "outputId": "1999674f-9c43-4edd-924d-700d0fcbcb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W/O Paraphrase Detection AUROC with gradients:  ([0.0, 0.0, 0.0, 1.0], [0.0, 0.25, 1.0, 1.0], 1.0)\n",
            "W/ Paraphrase Detection AUROC with gradients:  ([0.0, 0.0, 0.0, 1.0], [0.0, 0.25, 1.0, 1.0], 1.0)\n",
            "W/ Paraphrase Detection AUROC with gradients:  ([0.0, 0.0, 0.0, 1.0], [0.0, 0.25, 1.0, 1.0], 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Altering the text features and observing predictions***"
      ],
      "metadata": {
        "id": "0shLUDdp2HD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1- max_tokens=300, seed=1, temperature=0.8,top_p=0.1, n=1, frequency_penalty=-2"
      ],
      "metadata": {
        "id": "henYmPyj46uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=1, temperature=0.8,top_p=0.1, n=1, frequency_penalty=-2)\n",
        "print(AI_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV1EkhkZx23-",
        "outputId": "591e8ec7-738d-4c42-def7-a050625c0c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Trump has expressed his willingness to meet with North Korean leader Kim Jong Un to discuss denuclearization and peace on the Korean Peninsula.', 'such as steam generation, water heating, and thermal energy storage. These technologies use mirrors or lenses to concentrate sunlight onto a receiver, where the energy is converted into heat. The high temperatures achieved can be used in a variety of industrial processes, such as in the manufacturing, food, and chemical industries, as well as in power generation, desalination, and, in some cases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,', '...take action on the environmental impacts of the oil industry in the Hermosa Beach area.', 'The Bush administration then turned its attention to Iraq, arguing that the need to remove Saddam Hussein from power had become urgent.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "Paraphrased_responses_ht = [ParaphraseNLGHumanText.openai_response(i, \"gpt-4o\") for i in human_texts]"
      ],
      "metadata": {
        "id": "kQVaKxPn2r8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(human_texts)\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)\n",
        "print(\"Paraphrased human text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses_ht)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6DRrfoG3C2a",
        "outputId": "2b46481d-69e4-4d10-b4df-421568a46cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.22256025671958923, 0.1006336361169815, 0.9964590668678284, 0.0139209795743227]\n",
            "AI text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9993743300437927, 0.9994033575057983, 0.9994151592254639, 0.9992238283157349]\n",
            "Paraphrased text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.99943608045578, 0.9828252196311951, 0.9995008707046509, 0.9991456270217896]\n",
            "Paraphrased human text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.016393421217799187, 0.054396893829107285, 0.9943987727165222, 0.010598534718155861]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 2- max_tokens=300, seed=2, temperature=1,top_p=0.1, n=1, frequency_penalty=-1"
      ],
      "metadata": {
        "id": "Rsh0RWam5BMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=1, temperature=0.8,top_p=0.1, n=1, frequency_penalty=-1)\n",
        "# print(AI_texts)\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(human_texts)\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)\n",
        "print(\"Paraphrased human text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses_ht)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucu6z6EK4wXe",
        "outputId": "5ef53d33-2554-4ac6-af50-1ae1a7da2b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.22256025671958923, 0.1006336361169815, 0.9964590668678284, 0.0139209795743227]\n",
            "AI text predictions from the detector\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9994514584541321, 0.9983538389205933, 0.9994996786117554, 0.9992238283157349]\n",
            "Paraphrased text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.9995185136795044, 0.9972115159034729, 0.9994076490402222, 0.9992938041687012]\n",
            "Paraphrased human text predictions\n",
            "No of input instances fed to the detector 4\n",
            "Probability of text examples: [0.016393421217799187, 0.054396893829107285, 0.9943987727165222, 0.010598534718155861]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Camel-5b prerained model"
      ],
      "metadata": {
        "id": "l6DpLbF39VHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming detector is your model and tokenizer is already defined\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = \"cuda:0\"\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "detector_modelcard = \"Writer/camel-5b-hf\"\n",
        "#models - [Writer/camel-5b-hf,databricks/dolly-v2-3b]\n",
        "detector = GPT2Model.from_pretrained(detector_modelcard)"
      ],
      "metadata": {
        "id": "4ki519e_5QDf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d4819890f81c4f4290abd97d7a1a054b",
            "36beda90a5f14c4588f17d68fc48215c",
            "608d9f01ceca4d2a88e237593b8f6ebf",
            "8d06a2acb0214e589fba41b986eebe07",
            "a2402db6214f443ab3dec44222abf3ba",
            "0eb74cceb53e489eb6959f18fa3bb57f",
            "dcb7e83d40f44c378e8b6483b6fd1261",
            "07c1a9ad4941496b85234469a7526a5f",
            "049c46c59bbd456e87fe6ad81c529acd",
            "122e1abb88124360b6e13e5dd8a436ae",
            "fb2929d804c04a41831262e6dc082e8b"
          ]
        },
        "outputId": "bb3ce81b-6cf7-4fa7-8d2c-391e7e7f0d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4819890f81c4f4290abd97d7a1a054b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detector_config = AutoConfig.from_pretrained(detector_modelcard)\n",
        "print(detector_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq3qCX7S9tOW",
        "outputId": "4e8b76e1-30d7-41c7-a4dc-055a57d01750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"_name_or_path\": \"Writer/camel-5b-hf\",\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.01,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_embd\": 4096,\n",
            "  \"n_head\": 32,\n",
            "  \"n_inner\": 16384,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50258\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(detector_modelcard)\n",
        "detector.eval()\n",
        "# detector.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwuH7Gj34IBo",
        "outputId": "14902f24-14c6-4a42-cb9d-508c1c8aeb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Model(\n",
              "  (wte): Embedding(50258, 4096)\n",
              "  (wpe): Embedding(2048, 4096)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (h): ModuleList(\n",
              "    (0-23): 24 x GPT2Block(\n",
              "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): GPT2Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): GPT2MLP(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (act): GELUActivation()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLGClass:\n",
        "  def __init__(self, api_key, raw_texts, instruction, model):\n",
        "    openai.api_key = api_key\n",
        "    self.human_texts = raw_texts\n",
        "    self.instruction = instruction\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def generate_text_completion(self, max_tokens, seed, temperature,top_p, n, frequency_penalty):\n",
        "    ai_texts = []\n",
        "    for text in self.human_texts:\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model=self.model,\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": self.instruction},\n",
        "              {\"role\": \"user\", \"content\": text}\n",
        "          ],\n",
        "          max_tokens=max_tokens,\n",
        "          seed=seed,\n",
        "          temperature=temperature,\n",
        "          top_p=top_p,\n",
        "          n=n, #iterations\n",
        "          stop=None,\n",
        "          frequency_penalty=frequency_penalty\n",
        "      )\n",
        "      ai_texts.append(response.choices[0].message['content'].strip())\n",
        "    return ai_texts\n",
        "\n"
      ],
      "metadata": {
        "id": "7T6ToUbd4b7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without gradient\n",
        "# input will contain the raw output scores(logits in this case) of the probabilities generated by the detector\n",
        "class textPrediction:\n",
        "  # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  @staticmethod\n",
        "  def textInputFun(examples):\n",
        "    with torch.no_grad():\n",
        "      inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "      if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        detector.resize_token_embeddings(len(tokenizer))\n",
        "      input = {k: v.to(device) for k, v in inputs.items()}\n",
        "      output_prob = F.log_softmax(detector(**inputs).logits, -1)[:, 0].exp().tolist()\n",
        "    print(\"No of input instances fed to the detector\", len(examples))\n",
        "    print(f\"Probability of text examples: {output_prob}\")\n",
        "    return output_prob"
      ],
      "metadata": {
        "id": "bMflRd5_7-vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"your-openai-key\"\n",
        "model = \"gpt-3.5-turbo\"\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=0, temperature=0.5,top_p=0.1, n=1, frequency_penalty=0)\n",
        "print(AI_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOeVQAaV4odG",
        "outputId": "9375362b-3f84-4c34-a0af-95530468ba38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Trump has expressed his willingness to meet with North Korean leader Kim Jong Un in the near future.', 'such as steam generation, water heating, and thermal energy storage. These technologies use mirrors or lenses to concentrate sunlight onto a receiver, where the energy is converted into heat. This heat can then be used in various industrial processes such as food processing, chemical production, and desalination. By harnessing the power of the sun, these solar concentrating technologies offer a sustainable and environmentally friendly alternative to traditional fossil fuel-based heating systems.', '...action against offshore oil drilling in California. The bill aims to protect the coastal communities and marine environment from the potential risks associated with oil drilling activities.', 'The Bush administration then turned its attention to Iraq, arguing that the need to remove Saddam Hussein from power had become urgent.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ParaphraseNLG:\n",
        "  @staticmethod\n",
        "  def openai_response(text, openai_model):\n",
        "    instruction = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    prompt={\"role\": \"user\", \"content\": text}\n",
        "    messages = [instruction, prompt]\n",
        "    k_wargs = { \"messages\": messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r"
      ],
      "metadata": {
        "id": "g6Wgv_I0_RIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#These are the paraphrased responses generated from GPT 3.5\n",
        "#Using gpt-4o to check the semantical differences from human text\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]"
      ],
      "metadata": {
        "id": "YSmf-9Gn4_aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(human_texts)\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)"
      ],
      "metadata": {
        "id": "p8I7dTYo5HQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import openai\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Instruction and text inputs (Paper Reference -[Learning toWatermark LLM-generated Text via Reinforcement Learning] \"Examples of C4 on Llama2-7b dataset\", Examples of C4 on OPT-1.3b dataset)\n",
        "instruction = \"You will assist in completing the given text examples:\"\n",
        "raw_texts = [\"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald...\",\n",
        "             \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications...\",\n",
        "             \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take...\",\n",
        "             \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent...\"]\n",
        "human_texts = [\"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald Trump and North Korean leader Kim Jong Un will meet within weeks, White House spokesman Raj Shah said on Monday, as the administration seeks a deal with Pyongyang to rid North Korea of its nuclear weapons programme. The meeting would be the first of its kind between leaders of the countries, and potentially set in motion a long-awaited ”peace dialogue” that could lead to the first meeting of Moon Jae-in, the newly elected leader of South Korea, with the reclusive North. Advertisement For the latest headlines, follow our Google News channel online or via the app. The visit of Kim to South Korea for a summit on April\",\n",
        "               \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "               \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take a bite out of the cost of living in the South Bay. The bill, AB 191, would allow cities to create a Community Facilities District, which would allow them to issue bonds to pay for infrastructure improvements. The bill would also allow cities to create a Community Facilities District to pay for infrastructure improvements. “The South Bay is a great place to live, work and raise a family, but the cost of living is too high,” Muratsuchi said in a statement. “AB 191 will help cities in the South Bay and across the state\",\n",
        "               \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"]\n",
        "\n",
        "# Setup\n",
        "device = \"cuda:0\"\n",
        "detector_modelcard = \"databricks/dolly-v2-3b\"\n",
        "detector = AutoModelForSequenceClassification.from_pretrained(detector_modelcard)\n",
        "tokenizer = AutoTokenizer.from_pretrained(detector_modelcard)\n",
        "# detector.to(device)\n",
        "detector.eval()\n",
        "\n",
        "# Define classes\n",
        "class NLGClass:\n",
        "    def __init__(self, api_key, raw_texts, instruction, model):\n",
        "        openai.api_key = api_key\n",
        "        self.human_texts = raw_texts\n",
        "        self.instruction = instruction\n",
        "        self.model = model\n",
        "\n",
        "    def generate_text_completion(self, max_tokens, seed, temperature, top_p, n, frequency_penalty):\n",
        "        ai_texts = []\n",
        "        for text in self.human_texts:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.instruction},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "                max_tokens=max_tokens,\n",
        "                seed=seed,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                n=n,  # iterations\n",
        "                stop=None,\n",
        "                frequency_penalty=frequency_penalty\n",
        "            )\n",
        "            ai_texts.append(response.choices[0].message['content'].strip())\n",
        "        return ai_texts\n",
        "\n",
        "class textPrediction:\n",
        "    @staticmethod\n",
        "    def textInputFun(examples):\n",
        "        with torch.no_grad():\n",
        "            # inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "            # input = {k: v.to(device) for k, v in inputs.items()}\n",
        "            # output_prob = F.log_softmax(detector(**inputs).logits, -1)[:, 0].exp().tolist()\n",
        "            inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = detector(**inputs)\n",
        "            logits = outputs.logits\n",
        "            output_prob = F.log_softmax(logits, -1)[:, 0].exp().tolist()\n",
        "        print(\"No of input instances fed to the detector\", len(examples))\n",
        "        print(f\"Probability of text examples: {output_prob}\")\n",
        "        return output_prob\n",
        "\n",
        "class ParaphraseNLG:\n",
        "  @staticmethod\n",
        "  def openai_response(text, openai_model):\n",
        "    instruction = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "    prompt={\"role\": \"user\", \"content\": text}\n",
        "    messages = [instruction, prompt]\n",
        "    k_wargs = { \"messages\": messages, \"model\": openai_model}\n",
        "    r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "    return r\n",
        "\n",
        "# Example usage\n",
        "api_key = \"your-openai-key\"\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=0, temperature=0.5, top_p=0.1, n=1, frequency_penalty=0)\n",
        "print(AI_texts)\n",
        "\n",
        "# These are the paraphrased responses generated from GPT 3.5\n",
        "# Using gpt-4o to check the semantical differences from human text\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "\n",
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(raw_texts)\n",
        "\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)\n"
      ],
      "metadata": {
        "id": "z8mk_gWj5xm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import openai\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Instruction and text inputs\n",
        "instruction = \"You will assist in completing the given text examples:\"\n",
        "raw_texts = [\n",
        "    \"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald...\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications...\",\n",
        "    \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take...\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent...\"\n",
        "]\n",
        "human_texts = [\n",
        "    \"The White House has said the first meeting ever between sitting U.S. and North Korean leaders could take place in the coming weeks. Washington: President Donald Trump and North Korean leader Kim Jong Un will meet within weeks, White House spokesman Raj Shah said on Monday, as the administration seeks a deal with Pyongyang to rid North Korea of its nuclear weapons programme. The meeting would be the first of its kind between leaders of the countries, and potentially set in motion a long-awaited ”peace dialogue” that could lead to the first meeting of Moon Jae-in, the newly elected leader of South Korea, with the reclusive North. Advertisement For the latest headlines, follow our Google News channel online or via the app. The visit of Kim to South Korea for a summit on April\",\n",
        "    \"Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste\",\n",
        "    \"An undated aerial photo during the oil boom era in Hermosa Beach. South Bay Assemblyman Al Muratsuchi on Friday announced a bill designed to take a bite out of the cost of living in the South Bay. The bill, AB 191, would allow cities to create a Community Facilities District, which would allow them to issue bonds to pay for infrastructure improvements. The bill would also allow cities to create a Community Facilities District to pay for infrastructure improvements. “The South Bay is a great place to live, work and raise a family, but the cost of living is too high,” Muratsuchi said in a statement. “AB 191 will help cities in the South Bay and across the state\",\n",
        "    \"The Bush administration then turned its attention to Iraq, and argued the need to remove Saddam Hussein from power in Iraq had become urgent. Among the stated reasons were that Saddam's regime had tried to acquire nuclear material and had not properly accounted for biological and chemical material it was known to have previously possessed, and believed to still maintain. Both the possession of these weapons of mass destruction (WMD), and the failure to account for them, would violate the U.N. sanctions. The assertion about WMD was hotly advanced by the Bush administration from the beginning, but other major powers including China, France, Germany, and Russia remained unconvinced that Iraq was a threat and refused to allow passage of a UN Security Council resolution to authorize the use of force. Iraq permitted UN weapon inspectors in November 2002, who were continuing their work to assess the WMD claim when the Bush administration decided to proceed with war without UN authorization and told the inspectors to leave the\"\n",
        "]\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "detector_modelcard = \"databricks/dolly-v2-3b\"\n",
        "detector = AutoModelForSequenceClassification.from_pretrained(detector_modelcard)\n",
        "tokenizer = AutoTokenizer.from_pretrained(detector_modelcard)\n",
        "\n",
        "# Ensure the padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    detector.resize_token_embeddings(len(tokenizer))\n",
        "    detector.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "detector.to(device)\n",
        "detector.eval()\n",
        "\n",
        "# Define classes\n",
        "class NLGClass:\n",
        "    def __init__(self, api_key, raw_texts, instruction, model):\n",
        "        openai.api_key = api_key\n",
        "        self.human_texts = raw_texts\n",
        "        self.instruction = instruction\n",
        "        self.model = model\n",
        "\n",
        "    def generate_text_completion(self, max_tokens, seed, temperature, top_p, n, frequency_penalty):\n",
        "        ai_texts = []\n",
        "        for text in self.human_texts:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.instruction},\n",
        "                    {\"role\": \"user\", \"content\": text}\n",
        "                ],\n",
        "                max_tokens=max_tokens,\n",
        "                seed=seed,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                n=n,  # iterations\n",
        "                stop=None,\n",
        "                frequency_penalty=frequency_penalty\n",
        "            )\n",
        "            ai_texts.append(response.choices[0].message['content'].strip())\n",
        "        return ai_texts\n",
        "\n",
        "class textPrediction:\n",
        "    @staticmethod\n",
        "    def textInputFun(examples):\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer(examples, padding=True, truncation=True, max_length=300, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = detector(**inputs)\n",
        "            logits = outputs.logits\n",
        "            output_prob = F.log_softmax(logits, -1)[:, 0].exp().tolist()\n",
        "        print(\"No of input instances fed to the detector\", len(examples))\n",
        "        print(f\"Probability of text examples: {output_prob}\")\n",
        "        return output_prob\n",
        "\n",
        "class ParaphraseNLG:\n",
        "    @staticmethod\n",
        "    def openai_response(text, openai_model):\n",
        "        instruction = {\"role\": \"system\", \"content\": \"Enhance the word choices in the sentence to sound more like that of a human.\"}\n",
        "        prompt = {\"role\": \"user\", \"content\": text}\n",
        "        messages = [instruction, prompt]\n",
        "        k_wargs = {\"messages\": messages, \"model\": openai_model}\n",
        "        r = openai.ChatCompletion.create(**k_wargs)['choices'][0].message.content\n",
        "        return r\n",
        "\n",
        "# Example usage\n",
        "api_key = \"your-openai-key\"\n",
        "model = \"gpt-3.5-turbo\"\n",
        "\n",
        "nlg_obj = NLGClass(api_key, raw_texts, instruction, model)\n",
        "AI_texts = nlg_obj.generate_text_completion(max_tokens=300, seed=0, temperature=0.5, top_p=0.1, n=1, frequency_penalty=0)\n",
        "print(AI_texts)\n",
        "\n",
        "# These are the paraphrased responses generated from GPT 3.5\n",
        "# Using gpt-4o to check the semantical differences from human text\n",
        "Paraphrased_responses = [ParaphraseNLG.openai_response(i, \"gpt-4o\") for i in AI_texts]\n",
        "\n",
        "print(\"Human text predictions from the detector\")\n",
        "human_preds = textPrediction.textInputFun(human_texts)\n",
        "\n",
        "print(\"AI text predictions from the detector\")\n",
        "ai_preds = textPrediction.textInputFun(AI_texts)\n",
        "\n",
        "print(\"Paraphrased text predictions\")\n",
        "paraphrased_preds = textPrediction.textInputFun(Paraphrased_responses)\n"
      ],
      "metadata": {
        "id": "wNeaJtPtA7ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z1awzdvQcZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}